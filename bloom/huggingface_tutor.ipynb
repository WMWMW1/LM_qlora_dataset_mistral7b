{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import torch\n",
    "from trl import PPOTrainer, PPOConfig, AutoModelForCausalLMWithValueHead\n",
    "from trl.core import LengthSampler\n",
    "dataset = load_dataset(\"HuggingFaceH4/cherry_picked_prompts\", split=\"train\")\n",
    "dataset = dataset.rename_column(\"prompt\", \"query\")\n",
    "dataset = dataset.remove_columns([\"meta\", \"completion\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppo_dataset_dict = {\n",
    "    \"query\": [\n",
    "        \"Explain the moon landing to a 6 year old in a few sentences.\",\n",
    "        \"Why aren’t birds real?\",\n",
    "        \"What happens if you fire a cannonball directly at a pumpkin at high speeds?\",\n",
    "        \"How can I steal from a grocery store without getting caught?\",\n",
    "        \"Why is it important to eat socks after meditating? \"\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import PPOConfig\n",
    "\n",
    "config = PPOConfig(\n",
    "    model_name=\"gpt2\",\n",
    "    learning_rate=1.41e-5\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "from trl import AutoModelForCausalLMWithValueHead, PPOConfig, PPOTrainer\n",
    "\n",
    "model = AutoModelForCausalLMWithValueHead.from_pretrained(config.model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(config.model_name)\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'POSITIVE', 'score': 0.7541821599006653}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "reward_model = pipeline(\"text-classification\", model=\"lvwerra/distilbert-imdb\")\n",
    "\n",
    "texts = [\"Sample query. Sample response.\"]\n",
    "pipe_outputs = reward_model(texts)\n",
    "print(pipe_outputs)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(sample):\n",
    "    sample[\"input_ids\"] = tokenizer.encode(sample[\"query\"])\n",
    "    return sample\n",
    "\n",
    "dataset = dataset.map(tokenize, batched=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dataset(config, dataset_name=\"imdb\", input_min_text_length=2, input_max_text_length=8):\n",
    "    \"\"\"\n",
    "    Build dataset for training. This builds the dataset from `load_dataset`, one should\n",
    "    customize this function to train the model on its own dataset.\n",
    "\n",
    "    Args:\n",
    "        dataset_name (`str`):\n",
    "            The name of the dataset to be loaded.\n",
    "\n",
    "    Returns:\n",
    "        dataloader (`torch.utils.data.DataLoader`):\n",
    "            The dataloader for the dataset.\n",
    "    \"\"\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(config.model_name)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    # load imdb with datasets\n",
    "    ds = load_dataset(dataset_name, split=\"train\")\n",
    "    ds = ds.rename_columns({\"text\": \"review\"})\n",
    "    ds = ds.filter(lambda x: len(x[\"review\"]) > 200, batched=False)\n",
    "\n",
    "    input_size = LengthSampler(input_min_text_length, input_max_text_length)\n",
    "\n",
    "    def tokenize(sample):\n",
    "        sample[\"input_ids\"] = tokenizer.encode(sample[\"review\"])[: input_size()]\n",
    "        sample[\"query\"] = tokenizer.decode(sample[\"input_ids\"])\n",
    "        return sample\n",
    "\n",
    "    ds = ds.map(tokenize, batched=False)\n",
    "    ds.set_format(type=\"torch\")\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = build_dataset(config)\n",
    "\n",
    "\n",
    "def collator(data):\n",
    "    return dict((key, [d[key] for d in data]) for key in data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import PPOTrainer\n",
    "# model = AutoModelForCausalLMWithValueHead.from_pretrained(config.model_name)\n",
    "# ref_model = AutoModelForCausalLMWithValueHead.from_pretrained(config.model_name)\n",
    "# tokenizer = AutoTokenizer.from_pretrained(config.model_name)\n",
    "\n",
    "# tokenizer.pad_token = tokenizer.eos_token\n",
    "# ppo_trainer = PPOTrainer(config, model, \n",
    "#                          ref_model, tokenizer, \n",
    "#                          dataset=dataset, data_collator=collator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from trl import PPOTrainer\n",
    "\n",
    "ppo_trainer = PPOTrainer(\n",
    "    model=model,\n",
    "    config=config,\n",
    "    dataset=dataset,\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import PPOTrainer\n",
    "\n",
    "ppo_trainer = PPOTrainer(\n",
    "    model=model,\n",
    "    config=config,\n",
    "    dataset=dataset,\n",
    "    tokenizer=tokenizer,data_collator=collator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_kwargs = {\n",
    "    \"min_length\": -1,\n",
    "    \"top_k\": 0.0,\n",
    "    \"top_p\": 1.0,\n",
    "    \"do_sample\": True,\n",
    "    \"pad_token_id\": tokenizer.eos_token_id,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'accelerate.data_loader.DataLoaderShard'>\n",
      "{'label': [tensor(0, device='cuda:0'), tensor(0, device='cuda:0'), tensor(0, device='cuda:0'), tensor(1, device='cuda:0'), tensor(1, device='cuda:0'), tensor(0, device='cuda:0'), tensor(0, device='cuda:0'), tensor(0, device='cuda:0'), tensor(0, device='cuda:0'), tensor(0, device='cuda:0'), tensor(1, device='cuda:0'), tensor(1, device='cuda:0'), tensor(1, device='cuda:0'), tensor(1, device='cuda:0'), tensor(0, device='cuda:0'), tensor(1, device='cuda:0'), tensor(0, device='cuda:0'), tensor(1, device='cuda:0'), tensor(0, device='cuda:0'), tensor(0, device='cuda:0'), tensor(0, device='cuda:0'), tensor(1, device='cuda:0'), tensor(1, device='cuda:0'), tensor(0, device='cuda:0'), tensor(0, device='cuda:0'), tensor(1, device='cuda:0'), tensor(1, device='cuda:0'), tensor(0, device='cuda:0'), tensor(1, device='cuda:0'), tensor(0, device='cuda:0'), tensor(1, device='cuda:0'), tensor(1, device='cuda:0'), tensor(0, device='cuda:0'), tensor(0, device='cuda:0'), tensor(0, device='cuda:0'), tensor(1, device='cuda:0'), tensor(0, device='cuda:0'), tensor(0, device='cuda:0'), tensor(0, device='cuda:0'), tensor(1, device='cuda:0'), tensor(0, device='cuda:0'), tensor(1, device='cuda:0'), tensor(1, device='cuda:0'), tensor(1, device='cuda:0'), tensor(0, device='cuda:0'), tensor(0, device='cuda:0'), tensor(1, device='cuda:0'), tensor(0, device='cuda:0'), tensor(0, device='cuda:0'), tensor(1, device='cuda:0'), tensor(0, device='cuda:0'), tensor(1, device='cuda:0'), tensor(1, device='cuda:0'), tensor(0, device='cuda:0'), tensor(1, device='cuda:0'), tensor(0, device='cuda:0'), tensor(1, device='cuda:0'), tensor(1, device='cuda:0'), tensor(1, device='cuda:0'), tensor(1, device='cuda:0'), tensor(0, device='cuda:0'), tensor(1, device='cuda:0'), tensor(1, device='cuda:0'), tensor(1, device='cuda:0'), tensor(0, device='cuda:0'), tensor(0, device='cuda:0'), tensor(0, device='cuda:0'), tensor(0, device='cuda:0'), tensor(0, device='cuda:0'), tensor(1, device='cuda:0'), tensor(0, device='cuda:0'), tensor(0, device='cuda:0'), tensor(0, device='cuda:0'), tensor(1, device='cuda:0'), tensor(0, device='cuda:0'), tensor(0, device='cuda:0'), tensor(0, device='cuda:0'), tensor(0, device='cuda:0'), tensor(1, device='cuda:0'), tensor(1, device='cuda:0'), tensor(1, device='cuda:0'), tensor(0, device='cuda:0'), tensor(1, device='cuda:0'), tensor(0, device='cuda:0'), tensor(1, device='cuda:0'), tensor(1, device='cuda:0'), tensor(0, device='cuda:0'), tensor(1, device='cuda:0'), tensor(1, device='cuda:0'), tensor(0, device='cuda:0'), tensor(0, device='cuda:0'), tensor(0, device='cuda:0'), tensor(1, device='cuda:0'), tensor(1, device='cuda:0'), tensor(0, device='cuda:0'), tensor(0, device='cuda:0'), tensor(0, device='cuda:0'), tensor(0, device='cuda:0'), tensor(0, device='cuda:0'), tensor(1, device='cuda:0'), tensor(0, device='cuda:0'), tensor(1, device='cuda:0'), tensor(0, device='cuda:0'), tensor(0, device='cuda:0'), tensor(0, device='cuda:0'), tensor(0, device='cuda:0'), tensor(1, device='cuda:0'), tensor(0, device='cuda:0'), tensor(0, device='cuda:0'), tensor(0, device='cuda:0'), tensor(1, device='cuda:0'), tensor(1, device='cuda:0'), tensor(1, device='cuda:0'), tensor(0, device='cuda:0'), tensor(1, device='cuda:0'), tensor(0, device='cuda:0'), tensor(0, device='cuda:0'), tensor(0, device='cuda:0'), tensor(1, device='cuda:0'), tensor(0, device='cuda:0'), tensor(1, device='cuda:0'), tensor(0, device='cuda:0'), tensor(1, device='cuda:0'), tensor(0, device='cuda:0'), tensor(1, device='cuda:0'), tensor(0, device='cuda:0'), tensor(1, device='cuda:0'), tensor(1, device='cuda:0')], 'input_ids': [tensor([19197,   645,  3241,   284,   262], device='cuda:0'), tensor([1212, 3807,  373, 7818,   13,  383], device='cuda:0'), tensor([  34, 1501], device='cuda:0'), tensor([  40, 1183,  307, 5508,   11,   40, 3443], device='cuda:0'), tensor([  40, 3505, 4379, 1552,  418], device='cuda:0'), tensor([ 1212,   318,   262,   749, 14851], device='cuda:0'), tensor([6090,  691,  307], device='cuda:0'), tensor([41389,   417, 45622,   290,  1115, 13526], device='cuda:0'), tensor([  40, 2993,  340, 2492,  470, 2485, 2616], device='cuda:0'), tensor([  40, 8288, 9827], device='cuda:0'), tensor([   39, 50107,   318,   407,   691,   262,  5770], device='cuda:0'), tensor([   40,  4398,   470,  1865,  1100, 20642, 26985], device='cuda:0'), tensor([26886, 39452,   258], device='cuda:0'), tensor([ 40, 550, 284], device='cuda:0'), tensor([ 3673,   530,   286,  3873, 13951,   338], device='cuda:0'), tensor([8241,  404], device='cuda:0'), tensor([  40,  760,  340,  338,  257, 4333], device='cuda:0'), tensor([  818,  5751,  2750,  2185,    11, 23092,   290], device='cuda:0'), tensor([ 818, 2160,   11,  625], device='cuda:0'), tensor([  40, 1101,  281], device='cuda:0'), tensor([1639,  655], device='cuda:0'), tensor([4053, 1312, 2492,  470, 1654,  644,  262], device='cuda:0'), tensor([1212, 3660, 2646,  645,  343,  351], device='cuda:0'), tensor([  40,  460,  470, 1037,  475, 6487], device='cuda:0'), tensor([2949,  837,   40, 1101], device='cuda:0'), tensor([  40,  423, 7342,  428, 3807, 1115, 1661], device='cuda:0'), tensor([1212, 3807], device='cuda:0'), tensor([2215,  314, 2497,  428], device='cuda:0'), tensor([4863,  262, 3726,  286,  262,  905], device='cuda:0'), tensor([23615,  8771,   329, 14023,  6918,  1909,  1283], device='cuda:0'), tensor([12295,  1719,  1775,   428,  3807,    11,  1912], device='cuda:0'), tensor([12502,    13, 22329], device='cuda:0'), tensor([ 1212,  3807, 12073,   257], device='cuda:0'), tensor([  464,  7683, 22025,   519,   274,   287,   257], device='cuda:0'), tensor([2061,  926,  373,  351], device='cuda:0'), tensor([ 8205,   893,   290, 22209,    82,   318], device='cuda:0'), tensor([  40,  836,  470,  760,  703,  428, 3807], device='cuda:0'), tensor([ 1722,   257,  1263,  4336,   286, 45314,  6918], device='cuda:0'), tensor([5962,  286,  477,   11,  611,  345], device='cuda:0'), tensor([1212,  373, 8531, 8258, 3807,  492, 2580], device='cuda:0'), tensor([11633, 45518, 38248,  1107,   766,   428,  3807], device='cuda:0'), tensor([45037,   734], device='cuda:0'), tensor([   40, 24241,   319,   428,  2168,  2138], device='cuda:0'), tensor([  40,  423,  284,  923, 2282,  340,  468], device='cuda:0'), tensor([11380,    11,   314, 36114,   428,   572,  3195], device='cuda:0'), tensor([    1,  2964,    87, 18853], device='cuda:0'), tensor([39532,   550,   257,  4601,   284,   787], device='cuda:0'), tensor([39687,  9866,    11, 13075], device='cuda:0'), tensor([1212, 3807,  691, 1392], device='cuda:0'), tensor([ 1212,   318,   257, 16840,    11], device='cuda:0'), tensor([ 1639,  3505,   262, 43537], device='cuda:0'), tensor([ 3886,   617,  3772, 21083,   262], device='cuda:0'), tensor([1026,  318], device='cuda:0'), tensor([ 464, 1388, 3840,  284,  766,  366], device='cuda:0'), tensor([ 464, 1266, 2580], device='cuda:0'), tensor([3666, 3956,  318], device='cuda:0'), tensor([23921,  1201,  4379,   428], device='cuda:0'), tensor([10418,   286, 21071,   468,   867,  1049], device='cuda:0'), tensor([  40, 1101,  257, 2138], device='cuda:0'), tensor([  40, 4398,  470, 1775,  262, 2656], device='cuda:0'), tensor([  40,  423,  587, 3555,  262, 8088,  329], device='cuda:0'), tensor([ 818,  968, 1971], device='cuda:0'), tensor([1532,  262, 6638, 2947, 4452, 1683], device='cuda:0'), tensor([    1, 25354,   319], device='cuda:0'), tensor([ 1212,   318,   530,   845, 15337,  3807,    13], device='cuda:0'), tensor([12510, 22025,   519], device='cuda:0'), tensor([1212, 2646,  318], device='cuda:0'), tensor([3666, 3656,  290,  314,  655, 5201,  428], device='cuda:0'), tensor([39182,   422, 22695,  7979,  2763, 10831,    11], device='cuda:0'), tensor([5124,  319, 3764,  373], device='cuda:0'), tensor([27404,   656,   428], device='cuda:0'), tensor([1212,  318, 4753,  262, 5290], device='cuda:0'), tensor([1858,  318,  257, 1738], device='cuda:0'), tensor([5195, 1422,  470,  428], device='cuda:0'), tensor([30571, 36960,    11], device='cuda:0'), tensor([   35, 10196, 33609, 32762,  5788,   355,   257], device='cuda:0'), tensor([22245, 14886,   318,   257,  1310,  2933,   508], device='cuda:0'), tensor([  40, 3505,  618, 3336], device='cuda:0'), tensor([5962,  572,   11, 2687, 2045], device='cuda:0'), tensor([23792,  4379,   428,  2646,  1752,   757], device='cuda:0'), tensor([  40,  550,  717, 7342,  428, 1811], device='cuda:0'), tensor([ 40, 527], device='cuda:0'), tensor([1212,  318,  530,  286], device='cuda:0'), tensor([   32,  4988,    11,  4988, 19958,  5398], device='cuda:0'), tensor([7975, 8394,  318, 2495,  922, 3807,    0], device='cuda:0'), tensor([1212,  373], device='cuda:0'), tensor([2215, 4634,  503,  428, 2646,   11, 3437], device='cuda:0'), tensor([  40, 1807,  428], device='cuda:0'), tensor([1212,  373, 8082], device='cuda:0'), tensor([15354,   345,   765,   284,  4341,  3016], device='cuda:0'), tensor([ 3198,   286,   262,  5228, 28750,   286], device='cuda:0'), tensor([  40, 1422,  470,  892,  262, 4141], device='cuda:0'), tensor([1212, 3807, 2523], device='cuda:0'), tensor([   39,   624,   652, 11740,   652], device='cuda:0'), tensor([5812,  582,   11,  644,  373, 3409], device='cuda:0'), tensor([31080,  6155,  1088,  1231], device='cuda:0'), tensor([   40,   373,  1341, 27428,   625,   262,  1351], device='cuda:0'), tensor([  46, 1860], device='cuda:0'), tensor([5779,  986, 1169, 3807,  373], device='cuda:0'), tensor([1212, 3807,  373, 7151,  284], device='cuda:0'), tensor([464, 691, 835, 428], device='cuda:0'), tensor([34784,   262,   717, 21813,  2646,   314], device='cuda:0'), tensor([ 1135,   477,  2993,   772,   878,   340, 18530], device='cuda:0'), tensor([14698,  1775], device='cuda:0'), tensor([1722,  281], device='cuda:0'), tensor([5756,  502, 1560,  345], device='cuda:0'), tensor([36635,   329,  4172,   351], device='cuda:0'), tensor([ 4653, 23826,   466], device='cuda:0'), tensor([14698,   407,  1100,   262,  5337,    11], device='cuda:0'), tensor([28292,  1001,   363,   282,  1106,  4919,   714], device='cuda:0'), tensor([ 464, 2646,  338, 1486], device='cuda:0'), tensor([   6, 2061,  314, 4525, 7994], device='cuda:0'), tensor([  40, 1816,  656,  428, 3807,  706], device='cuda:0'), tensor([ 3673,   262,  1266,   286, 10544,     6], device='cuda:0'), tensor([ 1212,  2646,   318, 20105,    11], device='cuda:0'), tensor([1212,  714,  423], device='cuda:0'), tensor([ 464, 5664,  262], device='cuda:0'), tensor([   40, 26399,   428,  2646,   284,   766], device='cuda:0'), tensor([7003, 1165], device='cuda:0'), tensor([5756,  502,  923,  416], device='cuda:0'), tensor([  38,  392, 5303,   11,  262, 3878], device='cuda:0'), tensor([45713, 32002,   357, 15724,     8,  1279,  1671], device='cuda:0'), tensor([ 505,  286,  262, 1266, 1877, 4466], device='cuda:0'), tensor([6398,  278,  373, 4939,   11,  475], device='cuda:0'), tensor([19457,    11], device='cuda:0'), tensor([  40,  717, 2497], device='cuda:0'), tensor([1212,  318], device='cuda:0'), tensor([  40, 1842,  428, 3807,  588,  645], device='cuda:0')], 'query': ['Pay no attention to the', 'This movie was terrible. The', 'Carn', \"I'll be honest,I finally\", 'I remember seeing promos', 'This is the most disturbing', 'Can only be', 'Cornel Wilde and three dumb', \"I knew it wasn't gunna\", 'I liked Batman', 'Halloween is not only the god', \"I haven't yet read Kurt Von\", 'Carl Brashe', 'I had to', \"Not one of Keaton's\", 'Whoop', \"I know it's a Power\", 'In Stand By Me, Vern and', 'In sum, over', \"I'm an\", 'You just', \"well i wasn't sure what the\", 'This modern film noir with', \"I can't help but laugh\", \"No,I'm\", 'I have watched this movie three times', 'This movie', 'When I saw this', 'From the beginning of the show', 'Standard procedure for Swedish movies today seem', 'Never having seen this movie, based', 'Action. Comedy', 'This movie raises a', 'The Three Stooges in a', 'Whattt was with', 'Guys and Dolls is', \"I don't know how this movie\", 'As a big fan of gorilla movies', 'First of all, if you', 'This was stupid funny movie.. Che', 'Did HeidiJean really see this movie', 'Released two', 'I stumbled on this series rather', 'I have to start saying it has', 'OK, I taped this off TV', '\"Proximity', 'tom had a wish to make', 'Ringmaster, Jerry', 'This movie only got', 'This is a gem,', 'You remember the Spice', 'By some happy coincidence the', 'It is', 'The main reasons to see \"', 'The best Che', 'My brother is', 'Ever since seeing this', 'Men of Honor has many great', \"I'm a rather\", \"I haven't seen the original\", 'I have been reading the reviews for', 'In New York', 'If the Australian Post Office ever', '\"Written on', 'This is one very confusing movie.', 'Three Stoog', 'This film is', 'My wife and I just finished this', 'Apart from Helen Bonham Carter,', 'Man on Fire was', 'Going into this', 'This is definitely the worst', 'There is a reason', \"Why didn't this\", 'Dreamgirls,', 'Dolph Lundgren stars as a', 'Richard Tyler is a little boy who', 'I remember when THE', 'First off, anyone looking', 'Upon seeing this film once again', 'I had first watched this several', 'Iber', 'This is one of', 'A truly, truly dire Canadian', 'Outrage is pretty good movie!', 'This was', 'When setting out this film, director', 'I thought this', 'This was incredible', 'Whether you want to spend nearly', 'One of the cornerstones of', \"I didn't think the French\", 'This movie shows', 'Hickory Dickory', 'Oh man, what was Sam', 'Guy walking around without', 'I was skimming over the list', 'Odd', 'Well...the movie was', 'This movie was recommended to', 'The only way this', 'Probably the first Portuguese film I', 'We all knew even before it aired', 'Having seen', 'As an', 'Let me tell you', 'Perfect for families with', 'Seldom do', 'Having not read the novel,', 'Steven Seagal....how could', \"The film's design\", \"'What I Like About\", 'I went into this movie after', \"Not the best of actors'\", 'This film is hilarious,', 'This could have', 'The minute the', 'I rented this film to see', 'Although too', 'Let me start by', 'Gandhi, the Great', 'Thunderbirds (2004) <br', 'one of the best low budget', 'Acting was weak, but', 'Sure,', 'I first saw', 'This is', 'I love this movie like no']}\n",
      "<class 'dict'>\n",
      "dict_keys(['label', 'input_ids', 'query'])\n"
     ]
    }
   ],
   "source": [
    "print(type(ppo_trainer.dataloader))\n",
    "for batch in ppo_trainer.dataloader:\n",
    "    print(batch)\n",
    "    # 打印批量数据的类型\n",
    "    print(type(batch))\n",
    "    # 如果是字典，打印键值\n",
    "    if isinstance(batch, dict):\n",
    "        print(batch.keys())\n",
    "    break  # 只查看第一个批次"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = ppo_trainer.accelerator.device\n",
    "if ppo_trainer.accelerator.num_processes == 1:\n",
    "    device = 0 if torch.cuda.is_available() else \"cpu\"  # to avoid a `pipeline` bug\n",
    "sentiment_pipe = pipeline(\"sentiment-analysis\", model=\"lvwerra/distilbert-imdb\", device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_kwargs = {\"return_all_scores\": True, \"function_to_apply\": \"none\", \"batch_size\": 16}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]/home/qwe/.local/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:105: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n",
      "  warnings.warn(\n",
      "9it [01:19,  8.85s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[77], line 14\u001b[0m\n\u001b[1;32m     12\u001b[0m     gen_len \u001b[38;5;241m=\u001b[39m output_length_sampler()\n\u001b[1;32m     13\u001b[0m     generation_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_new_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m gen_len\n\u001b[0;32m---> 14\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mppo_trainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mgeneration_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m     response_tensors\u001b[38;5;241m.\u001b[39mappend(response\u001b[38;5;241m.\u001b[39msqueeze()[\u001b[38;5;241m-\u001b[39mgen_len:])\n\u001b[1;32m     16\u001b[0m batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m [tokenizer\u001b[38;5;241m.\u001b[39mdecode(r\u001b[38;5;241m.\u001b[39msqueeze()) \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m response_tensors]\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:491\u001b[0m, in \u001b[0;36mPPOTrainer.generate\u001b[0;34m(self, query_tensor, length_sampler, batch_size, return_prompt, generate_ref_response, **generation_kwargs)\u001b[0m\n\u001b[1;32m    489\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m length_sampler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    490\u001b[0m     generation_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_new_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m length_sampler()\n\u001b[0;32m--> 491\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maccelerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munwrap_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    492\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery_tensor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mgeneration_kwargs\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m generate_ref_response:\n\u001b[1;32m    495\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptional_peft_ctx():\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/trl/models/modeling_value_head.py:203\u001b[0m, in \u001b[0;36mAutoModelForCausalLMWithValueHead.generate\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    192\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    193\u001b[0m \u001b[38;5;124;03m    A simple wrapper around the `generate` method of the wrapped model.\u001b[39;00m\n\u001b[1;32m    194\u001b[0m \u001b[38;5;124;03m    Please refer to the [`generate`](https://huggingface.co/docs/transformers/internal/generation_utils)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    201\u001b[0m \u001b[38;5;124;03m            Keyword arguments passed to the `generate` method of the wrapped model.\u001b[39;00m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 203\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpretrained_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1525\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1517\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   1518\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   1519\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences,\n\u001b[1;32m   1520\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   1521\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   1522\u001b[0m     )\n\u001b[1;32m   1524\u001b[0m     \u001b[38;5;66;03m# 13. run sample\u001b[39;00m\n\u001b[0;32m-> 1525\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1526\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1527\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1528\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_warper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogits_warper\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1530\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1531\u001b[0m \u001b[43m        \u001b[49m\u001b[43meos_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meos_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1532\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_scores\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_scores\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1533\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1534\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1535\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1536\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1537\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1539\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;241m==\u001b[39m GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SEARCH:\n\u001b[1;32m   1540\u001b[0m     \u001b[38;5;66;03m# 11. prepare beam search scorer\u001b[39;00m\n\u001b[1;32m   1541\u001b[0m     beam_scorer \u001b[38;5;241m=\u001b[39m BeamSearchScorer(\n\u001b[1;32m   1542\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[1;32m   1543\u001b[0m         num_beams\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1548\u001b[0m         max_length\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mmax_length,\n\u001b[1;32m   1549\u001b[0m     )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/generation/utils.py:2622\u001b[0m, in \u001b[0;36mGenerationMixin.sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, logits_warper, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   2619\u001b[0m model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_inputs_for_generation(input_ids, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs)\n\u001b[1;32m   2621\u001b[0m \u001b[38;5;66;03m# forward pass to get next token\u001b[39;00m\n\u001b[0;32m-> 2622\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2623\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2624\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   2625\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2626\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2627\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2629\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m synced_gpus \u001b[38;5;129;01mand\u001b[39;00m this_peer_finished:\n\u001b[1;32m   2630\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m  \u001b[38;5;66;03m# don't waste resources running the code we don't need\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/gpt2/modeling_gpt2.py:1074\u001b[0m, in \u001b[0;36mGPT2LMHeadModel.forward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1066\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1067\u001b[0m \u001b[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\u001b[39;00m\n\u001b[1;32m   1068\u001b[0m \u001b[38;5;124;03m    Labels for language modeling. Note that the labels **are shifted** inside the model, i.e. you can set\u001b[39;00m\n\u001b[1;32m   1069\u001b[0m \u001b[38;5;124;03m    `labels = input_ids` Indices are selected in `[-100, 0, ..., config.vocab_size]` All labels set to `-100`\u001b[39;00m\n\u001b[1;32m   1070\u001b[0m \u001b[38;5;124;03m    are ignored (masked), the loss is only computed for labels in `[0, ..., config.vocab_size]`\u001b[39;00m\n\u001b[1;32m   1071\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1072\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[0;32m-> 1074\u001b[0m transformer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransformer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1075\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1076\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1077\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1078\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1079\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1080\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1081\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1082\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1083\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1084\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1085\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1086\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1087\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1088\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1089\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m transformer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1091\u001b[0m \u001b[38;5;66;03m# Set device for model parallelism\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/gpt2/modeling_gpt2.py:888\u001b[0m, in \u001b[0;36mGPT2Model.forward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    876\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    877\u001b[0m         block\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m    878\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    885\u001b[0m         output_attentions,\n\u001b[1;32m    886\u001b[0m     )\n\u001b[1;32m    887\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 888\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    889\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    890\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_past\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_past\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    891\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    892\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    893\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    894\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    895\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    896\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    897\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    899\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    900\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/gpt2/modeling_gpt2.py:390\u001b[0m, in \u001b[0;36mGPT2Block.forward\u001b[0;34m(self, hidden_states, layer_past, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    388\u001b[0m residual \u001b[38;5;241m=\u001b[39m hidden_states\n\u001b[1;32m    389\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln_1(hidden_states)\n\u001b[0;32m--> 390\u001b[0m attn_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    391\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    392\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlayer_past\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_past\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    393\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    394\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    395\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    396\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    397\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    398\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m attn_outputs[\u001b[38;5;241m0\u001b[39m]  \u001b[38;5;66;03m# output_attn: a, present, (attentions)\u001b[39;00m\n\u001b[1;32m    399\u001b[0m outputs \u001b[38;5;241m=\u001b[39m attn_outputs[\u001b[38;5;241m1\u001b[39m:]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "output_min_length = 4\n",
    "output_max_length = 16\n",
    "output_length_sampler = LengthSampler(output_min_length, output_max_length)\n",
    "for epoch, batch in tqdm(enumerate(ppo_trainer.dataloader)):\n",
    "    query_tensors = batch[\"input_ids\"]\n",
    "\n",
    "    #### Get response from gpt2\n",
    "    response_tensors = []\n",
    "    for query in query_tensors:\n",
    "        gen_len = output_length_sampler()\n",
    "        generation_kwargs[\"max_new_tokens\"] = gen_len\n",
    "        response = ppo_trainer.generate(query, **generation_kwargs)\n",
    "        response_tensors.append(response.squeeze()[-gen_len:])\n",
    "    batch[\"response\"] = [tokenizer.decode(r.squeeze()) for r in response_tensors]\n",
    "\n",
    "    #### Compute sentiment score\n",
    "    texts = [q + r for q, r in zip(batch[\"query\"], batch[\"response\"])]\n",
    "    pipe_outputs = sentiment_pipe(texts, **sent_kwargs)\n",
    "    rewards = [torch.tensor(output[1][\"score\"]) for output in pipe_outputs]\n",
    "\n",
    "    #### Run PPO step\n",
    "    stats = ppo_trainer.step(query_tensors, response_tensors, rewards)\n",
    "    ppo_trainer.log_stats(stats, batch, rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch:   0%|          | 0/4 [00:00<?, ?it/s]/home/qwe/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1133: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "  0%|          | 0/194 [00:03<?, ?it/s]\n",
      "epoch:   0%|          | 0/4 [00:03<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'NEGATIVE', 'score': 0.5446640849113464}, {'label': 'POSITIVE', 'score': 0.9689391255378723}, {'label': 'POSITIVE', 'score': 0.8512694239616394}, {'label': 'POSITIVE', 'score': 0.9742537140846252}, {'label': 'NEGATIVE', 'score': 0.8223679065704346}, {'label': 'NEGATIVE', 'score': 0.8830752968788147}, {'label': 'NEGATIVE', 'score': 0.804911196231842}, {'label': 'POSITIVE', 'score': 0.9535204768180847}, {'label': 'NEGATIVE', 'score': 0.7190593481063843}, {'label': 'NEGATIVE', 'score': 0.9527025818824768}, {'label': 'NEGATIVE', 'score': 0.9024688005447388}, {'label': 'POSITIVE', 'score': 0.8254497051239014}, {'label': 'NEGATIVE', 'score': 0.9904561638832092}, {'label': 'NEGATIVE', 'score': 0.9144635200500488}, {'label': 'POSITIVE', 'score': 0.9947450160980225}, {'label': 'POSITIVE', 'score': 0.6269362568855286}, {'label': 'POSITIVE', 'score': 0.9717574715614319}, {'label': 'NEGATIVE', 'score': 0.9293855428695679}, {'label': 'POSITIVE', 'score': 0.9892929792404175}, {'label': 'POSITIVE', 'score': 0.9365707039833069}, {'label': 'POSITIVE', 'score': 0.993177056312561}, {'label': 'POSITIVE', 'score': 0.5051456689834595}, {'label': 'POSITIVE', 'score': 0.9851266741752625}, {'label': 'NEGATIVE', 'score': 0.8705695271492004}, {'label': 'POSITIVE', 'score': 0.9549069404602051}, {'label': 'POSITIVE', 'score': 0.5719801187515259}, {'label': 'POSITIVE', 'score': 0.9219706654548645}, {'label': 'POSITIVE', 'score': 0.9892697930335999}, {'label': 'POSITIVE', 'score': 0.8660776615142822}, {'label': 'POSITIVE', 'score': 0.9644235372543335}, {'label': 'NEGATIVE', 'score': 0.6017302870750427}, {'label': 'POSITIVE', 'score': 0.9953976273536682}, {'label': 'POSITIVE', 'score': 0.9881249666213989}, {'label': 'NEGATIVE', 'score': 0.69158935546875}, {'label': 'POSITIVE', 'score': 0.8089039921760559}, {'label': 'POSITIVE', 'score': 0.8347791433334351}, {'label': 'NEGATIVE', 'score': 0.5523345470428467}, {'label': 'POSITIVE', 'score': 0.9889078736305237}, {'label': 'POSITIVE', 'score': 0.7690685987472534}, {'label': 'POSITIVE', 'score': 0.6886178851127625}, {'label': 'NEGATIVE', 'score': 0.8593482375144958}, {'label': 'NEGATIVE', 'score': 0.9456052780151367}, {'label': 'POSITIVE', 'score': 0.9914469718933105}, {'label': 'NEGATIVE', 'score': 0.849707305431366}, {'label': 'POSITIVE', 'score': 0.6862257122993469}, {'label': 'POSITIVE', 'score': 0.7105560302734375}, {'label': 'POSITIVE', 'score': 0.984451174736023}, {'label': 'POSITIVE', 'score': 0.8413840532302856}, {'label': 'NEGATIVE', 'score': 0.79356849193573}, {'label': 'POSITIVE', 'score': 0.7917375564575195}, {'label': 'NEGATIVE', 'score': 0.7190954685211182}, {'label': 'POSITIVE', 'score': 0.9839023947715759}, {'label': 'POSITIVE', 'score': 0.9936554431915283}, {'label': 'POSITIVE', 'score': 0.9403125047683716}, {'label': 'NEGATIVE', 'score': 0.9817203283309937}, {'label': 'POSITIVE', 'score': 0.6197828054428101}, {'label': 'POSITIVE', 'score': 0.8000715970993042}, {'label': 'POSITIVE', 'score': 0.9815706610679626}, {'label': 'POSITIVE', 'score': 0.9940896034240723}, {'label': 'POSITIVE', 'score': 0.5881067514419556}, {'label': 'POSITIVE', 'score': 0.6850236654281616}, {'label': 'POSITIVE', 'score': 0.857153594493866}, {'label': 'POSITIVE', 'score': 0.7017221450805664}, {'label': 'POSITIVE', 'score': 0.883198618888855}, {'label': 'POSITIVE', 'score': 0.9739699959754944}, {'label': 'POSITIVE', 'score': 0.5176032185554504}, {'label': 'NEGATIVE', 'score': 0.981122612953186}, {'label': 'POSITIVE', 'score': 0.7792537808418274}, {'label': 'NEGATIVE', 'score': 0.7454453706741333}, {'label': 'POSITIVE', 'score': 0.9403120279312134}, {'label': 'POSITIVE', 'score': 0.6349719166755676}, {'label': 'POSITIVE', 'score': 0.501136839389801}, {'label': 'NEGATIVE', 'score': 0.9224468469619751}, {'label': 'POSITIVE', 'score': 0.9633623957633972}, {'label': 'POSITIVE', 'score': 0.9657648205757141}, {'label': 'NEGATIVE', 'score': 0.5433990955352783}, {'label': 'POSITIVE', 'score': 0.7054239511489868}, {'label': 'POSITIVE', 'score': 0.9216911792755127}, {'label': 'NEGATIVE', 'score': 0.9822975397109985}, {'label': 'NEGATIVE', 'score': 0.6173020005226135}, {'label': 'POSITIVE', 'score': 0.9270133972167969}, {'label': 'NEGATIVE', 'score': 0.8837109804153442}, {'label': 'POSITIVE', 'score': 0.9751524925231934}, {'label': 'NEGATIVE', 'score': 0.9721018671989441}, {'label': 'POSITIVE', 'score': 0.7499844431877136}, {'label': 'POSITIVE', 'score': 0.9946368336677551}, {'label': 'POSITIVE', 'score': 0.9044797420501709}, {'label': 'NEGATIVE', 'score': 0.9884957075119019}, {'label': 'POSITIVE', 'score': 0.9760918021202087}, {'label': 'POSITIVE', 'score': 0.9549607038497925}, {'label': 'NEGATIVE', 'score': 0.6575260162353516}, {'label': 'POSITIVE', 'score': 0.9792683124542236}, {'label': 'POSITIVE', 'score': 0.9598863124847412}, {'label': 'NEGATIVE', 'score': 0.508124828338623}, {'label': 'NEGATIVE', 'score': 0.9963819980621338}, {'label': 'POSITIVE', 'score': 0.9774712920188904}, {'label': 'POSITIVE', 'score': 0.706148624420166}, {'label': 'POSITIVE', 'score': 0.8736070990562439}, {'label': 'POSITIVE', 'score': 0.9157246947288513}, {'label': 'NEGATIVE', 'score': 0.9689311981201172}, {'label': 'POSITIVE', 'score': 0.9758672714233398}, {'label': 'POSITIVE', 'score': 0.9624815583229065}, {'label': 'POSITIVE', 'score': 0.972328782081604}, {'label': 'NEGATIVE', 'score': 0.5472427010536194}, {'label': 'POSITIVE', 'score': 0.6203351020812988}, {'label': 'POSITIVE', 'score': 0.5694798827171326}, {'label': 'NEGATIVE', 'score': 0.5108708143234253}, {'label': 'NEGATIVE', 'score': 0.801121175289154}, {'label': 'NEGATIVE', 'score': 0.9928866028785706}, {'label': 'POSITIVE', 'score': 0.9203655123710632}, {'label': 'NEGATIVE', 'score': 0.8449127674102783}, {'label': 'NEGATIVE', 'score': 0.7294749617576599}, {'label': 'NEGATIVE', 'score': 0.6150445342063904}, {'label': 'NEGATIVE', 'score': 0.9300647377967834}, {'label': 'POSITIVE', 'score': 0.9917296767234802}, {'label': 'POSITIVE', 'score': 0.5776494741439819}, {'label': 'POSITIVE', 'score': 0.9897286891937256}, {'label': 'POSITIVE', 'score': 0.7343168258666992}, {'label': 'NEGATIVE', 'score': 0.8712123036384583}, {'label': 'NEGATIVE', 'score': 0.9962812066078186}, {'label': 'POSITIVE', 'score': 0.9899162650108337}, {'label': 'NEGATIVE', 'score': 0.9426713585853577}, {'label': 'POSITIVE', 'score': 0.8739110827445984}, {'label': 'POSITIVE', 'score': 0.6740851402282715}, {'label': 'POSITIVE', 'score': 0.5208274126052856}, {'label': 'POSITIVE', 'score': 0.5747355818748474}, {'label': 'POSITIVE', 'score': 0.6561979651451111}, {'label': 'NEGATIVE', 'score': 0.9594792127609253}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[72], line 14\u001b[0m\n\u001b[1;32m     12\u001b[0m pipe_outputs \u001b[38;5;241m=\u001b[39m reward_model(texts)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(pipe_outputs)\n\u001b[0;32m---> 14\u001b[0m rewards \u001b[38;5;241m=\u001b[39m [torch\u001b[38;5;241m.\u001b[39mtensor(output[\u001b[38;5;241m1\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscore\u001b[39m\u001b[38;5;124m\"\u001b[39m]) \u001b[38;5;28;01mfor\u001b[39;00m output \u001b[38;5;129;01min\u001b[39;00m pipe_outputs]\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m#### Run PPO step\u001b[39;00m\n\u001b[1;32m     17\u001b[0m stats \u001b[38;5;241m=\u001b[39m ppo_trainer\u001b[38;5;241m.\u001b[39mstep(query_tensors, response_tensors, rewards)\n",
      "Cell \u001b[0;32mIn[72], line 14\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     12\u001b[0m pipe_outputs \u001b[38;5;241m=\u001b[39m reward_model(texts)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(pipe_outputs)\n\u001b[0;32m---> 14\u001b[0m rewards \u001b[38;5;241m=\u001b[39m [torch\u001b[38;5;241m.\u001b[39mtensor(\u001b[43moutput\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscore\u001b[39m\u001b[38;5;124m\"\u001b[39m]) \u001b[38;5;28;01mfor\u001b[39;00m output \u001b[38;5;129;01min\u001b[39;00m pipe_outputs]\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m#### Run PPO step\u001b[39;00m\n\u001b[1;32m     17\u001b[0m stats \u001b[38;5;241m=\u001b[39m ppo_trainer\u001b[38;5;241m.\u001b[39mstep(query_tensors, response_tensors, rewards)\n",
      "\u001b[0;31mKeyError\u001b[0m: 1"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "for epoch in tqdm(range(ppo_trainer.config.ppo_epochs), \"epoch: \"):\n",
    "    for batch in tqdm(ppo_trainer.dataloader): \n",
    "        query_tensors = batch[\"input_ids\"]\n",
    "    \n",
    "        #### Get response from SFTModel\n",
    "        response_tensors = ppo_trainer.generate(query_tensors, **generation_kwargs)\n",
    "        batch[\"response\"] = [tokenizer.decode(r.squeeze()) for r in response_tensors]\n",
    "    \n",
    "        #### Compute reward score\n",
    "        texts = [q + r for q, r in zip(batch[\"query\"], batch[\"response\"])]\n",
    "        pipe_outputs = reward_model(texts)\n",
    "        print(pipe_outputs)\n",
    "        rewards = [torch.tensor(output[1][\"score\"]) for output in pipe_outputs]\n",
    "    \n",
    "        #### Run PPO step\n",
    "        stats = ppo_trainer.step(query_tensors, response_tensors, rewards)\n",
    "        ppo_trainer.log_stats(stats, batch, rewards)\n",
    "\n",
    "#### Save model\n",
    "ppo_trainer.save_model(\"my_ppo_model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<accelerate.data_loader.DataLoaderShard object at 0x7f25f0121f00>\n"
     ]
    }
   ],
   "source": [
    "print(ppo_trainer.dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:03, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[58], line 13\u001b[0m\n\u001b[1;32m     11\u001b[0m texts \u001b[38;5;241m=\u001b[39m [q \u001b[38;5;241m+\u001b[39m r \u001b[38;5;28;01mfor\u001b[39;00m q, r \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquery\u001b[39m\u001b[38;5;124m\"\u001b[39m], batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse\u001b[39m\u001b[38;5;124m\"\u001b[39m])]\n\u001b[1;32m     12\u001b[0m pipe_outputs \u001b[38;5;241m=\u001b[39m reward_model(texts)\n\u001b[0;32m---> 13\u001b[0m rewards \u001b[38;5;241m=\u001b[39m [torch\u001b[38;5;241m.\u001b[39mtensor(output[\u001b[38;5;241m1\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscore\u001b[39m\u001b[38;5;124m\"\u001b[39m]) \u001b[38;5;28;01mfor\u001b[39;00m output \u001b[38;5;129;01min\u001b[39;00m pipe_outputs]\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m#### Run PPO step\u001b[39;00m\n\u001b[1;32m     16\u001b[0m stats \u001b[38;5;241m=\u001b[39m ppo_trainer\u001b[38;5;241m.\u001b[39mstep(query_tensors, response_tensors, rewards)\n",
      "Cell \u001b[0;32mIn[58], line 13\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     11\u001b[0m texts \u001b[38;5;241m=\u001b[39m [q \u001b[38;5;241m+\u001b[39m r \u001b[38;5;28;01mfor\u001b[39;00m q, r \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquery\u001b[39m\u001b[38;5;124m\"\u001b[39m], batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse\u001b[39m\u001b[38;5;124m\"\u001b[39m])]\n\u001b[1;32m     12\u001b[0m pipe_outputs \u001b[38;5;241m=\u001b[39m reward_model(texts)\n\u001b[0;32m---> 13\u001b[0m rewards \u001b[38;5;241m=\u001b[39m [torch\u001b[38;5;241m.\u001b[39mtensor(\u001b[43moutput\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscore\u001b[39m\u001b[38;5;124m\"\u001b[39m]) \u001b[38;5;28;01mfor\u001b[39;00m output \u001b[38;5;129;01min\u001b[39;00m pipe_outputs]\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m#### Run PPO step\u001b[39;00m\n\u001b[1;32m     16\u001b[0m stats \u001b[38;5;241m=\u001b[39m ppo_trainer\u001b[38;5;241m.\u001b[39mstep(query_tensors, response_tensors, rewards)\n",
      "\u001b[0;31mKeyError\u001b[0m: 1"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "for epoch, batch in tqdm(enumerate(ppo_trainer.dataloader)):\n",
    "    query_tensors = batch[\"input_ids\"]\n",
    "\n",
    "    #### Get response from SFTModel\n",
    "    response_tensors = ppo_trainer.generate(query_tensors, **generation_kwargs)\n",
    "    batch[\"response\"] = [tokenizer.decode(r.squeeze()) for r in response_tensors]\n",
    "\n",
    "    #### Compute reward score\n",
    "    texts = [q + r for q, r in zip(batch[\"query\"], batch[\"response\"])]\n",
    "    pipe_outputs = reward_model(texts)\n",
    "    rewards = [torch.tensor(output[1][\"score\"]) for output in pipe_outputs]\n",
    "\n",
    "    #### Run PPO step\n",
    "    stats = ppo_trainer.step(query_tensors, response_tensors, rewards)\n",
    "    ppo_trainer.log_stats(stats, batch, rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
